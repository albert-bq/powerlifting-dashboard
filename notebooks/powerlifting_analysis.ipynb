{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b4f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 1\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# An√°lisis completo de datos de OpenPowerlifting\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# (Opcional) Carga remota a BigQuery\n",
    "USE_BIGQUERY = True\n",
    "\n",
    "if USE_BIGQUERY:\n",
    "    from google.cloud import bigquery\n",
    "    from google.oauth2 import service_account\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup directorios\n",
    "BASE_DIR = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "# Crear directorios si no existen\n",
    "for d in [DATA_DIR, RAW_DIR, PROCESSED_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Base directory: {BASE_DIR}\")\n",
    "print(\"‚úÖ Configuraci√≥n lista para an√°lisis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b8039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 2 (versi√≥n mejorada)\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Descarga y conversi√≥n eficiente a Parquet desde OpenPowerlifting\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Rutas\n",
    "RAW_DIR = Path(\"data/raw\")\n",
    "PROCESSED_DIR = Path(\"data/processed\")\n",
    "ZIP_URL = \"https://openpowerlifting.gitlab.io/opl-csv/files/openpowerlifting-latest.zip\"\n",
    "ZIP_PATH = RAW_DIR / \"openpowerlifting-latest.zip\"\n",
    "EXTRACT_PATH = RAW_DIR / \"extracted\"\n",
    "PARQUET_PATH = PROCESSED_DIR / \"powerlifting_raw.parquet\"\n",
    "\n",
    "# Crear carpetas necesarias\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EXTRACT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def download_and_convert():\n",
    "    \"\"\"Descargar ZIP, extraer CSV y convertir a Parquet.\"\"\"\n",
    "    \n",
    "    if PARQUET_PATH.exists():\n",
    "        print(f\"üì¶ Archivo Parquet ya existe: {PARQUET_PATH.name} ({round(os.path.getsize(PARQUET_PATH)/1e6, 1)} MB)\")\n",
    "        return PARQUET_PATH\n",
    "\n",
    "    if not ZIP_PATH.exists():\n",
    "        print(\"üì• Descargando datos de OpenPowerlifting...\")\n",
    "        r = requests.get(ZIP_URL, stream=True)\n",
    "        total = int(r.headers.get('content-length', 0))\n",
    "        with open(ZIP_PATH, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(\"‚úÖ Descarga completada.\")\n",
    "    \n",
    "    print(\"üóÇÔ∏è Extrayendo archivo ZIP...\")\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(EXTRACT_PATH)\n",
    "    \n",
    "    print(\"üîç Buscando archivo CSV...\")\n",
    "    csv_files = list(EXTRACT_PATH.glob(\"**/*.csv\"))\n",
    "    main_csv = [f for f in csv_files if 'openpowerlifting' in f.name and f.suffix == '.csv']\n",
    "    \n",
    "    if not main_csv:\n",
    "        raise FileNotFoundError(\"‚ùå No se encontr√≥ el archivo CSV principal.\")\n",
    "    \n",
    "    csv_path = main_csv[0]\n",
    "    print(f\"üìÑ CSV detectado: {csv_path.name}\")\n",
    "    \n",
    "    print(\"‚öôÔ∏è Cargando CSV y convirtiendo a Parquet...\")\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "    \n",
    "    print(f\"‚úÖ Datos cargados: {len(df):,} filas. Guardando como Parquet...\")\n",
    "    df.to_parquet(PARQUET_PATH, index=False)\n",
    "    print(f\"üéâ Parquet guardado en: {PARQUET_PATH} ({round(os.path.getsize(PARQUET_PATH)/1e6, 1)} MB)\")\n",
    "\n",
    "    return PARQUET_PATH\n",
    "\n",
    "# Ejecutar\n",
    "parquet_file = download_and_convert()\n",
    "print(f\"üì¶ Archivo Parquet listo: {parquet_file.name} ({round(os.path.getsize(parquet_file)/1e6, 1)} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd79ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 3 (mejorada)\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Carga de datos desde archivo Parquet\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo Parquet ya procesado\n",
    "parquet_path = Path(\"data/processed/powerlifting_raw.parquet\")\n",
    "\n",
    "# Validar existencia\n",
    "if not parquet_path.exists():\n",
    "    raise FileNotFoundError(\"‚ùå Archivo Parquet no encontrado. Ejecuta la Celda 2 para generarlo.\")\n",
    "\n",
    "# Cargar datos en memoria eficiente\n",
    "print(\"üìÇ Cargando datos desde archivo Parquet optimizado...\")\n",
    "df = pd.read_parquet(parquet_path)\n",
    "print(f\"‚úÖ Datos cargados correctamente:\")\n",
    "print(f\"   - Filas:     {df.shape[0]:,}\")\n",
    "print(f\"   - Columnas:  {df.shape[1]:,}\")\n",
    "print(f\"   - Columnas disponibles: {', '.join(df.columns[:8])}... (+{len(df.columns)-8} m√°s)\" if len(df.columns) > 8 else f\"   - Columnas: {df.columns.tolist()}\")\n",
    "print(f\"   - Memoria usada: {df.memory_usage(deep=True).sum() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf353c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 4 (mejorada)\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# An√°lisis exploratorio inicial\n",
    "\n",
    "print(\"üìä INFORMACI√ìN B√ÅSICA DEL DATASET\\n\" + \"-\"*40)\n",
    "print(f\"‚úîÔ∏è Filas         : {df.shape[0]:,}\")\n",
    "print(f\"‚úîÔ∏è Columnas      : {df.shape[1]:,}\")\n",
    "\n",
    "# Vista preliminar de datos\n",
    "print(\"\\nüîç PRIMERAS 3 FILAS:\")\n",
    "try:\n",
    "    display(df.head(3))  # Jupyter o Streamlit\n",
    "except Exception:\n",
    "    print(df.head(3).to_string(index=False))\n",
    "\n",
    "# Tipos de datos\n",
    "print(\"\\nüß† TIPOS DE DATOS:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(\"\\nTipos espec√≠ficos:\")\n",
    "print(df.dtypes.sort_values().astype(str).to_string())\n",
    "\n",
    "# Valores faltantes\n",
    "print(\"\\n‚ö†Ô∏è VALORES FALTANTES (TOP 10):\")\n",
    "missing = df.isna().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "if not missing.empty:\n",
    "    print(missing.head(10))\n",
    "else:\n",
    "    print(\"‚úÖ No hay valores faltantes significativos.\")\n",
    "\n",
    "# Estad√≠sticas descriptivas\n",
    "print(\"\\nüìà ESTAD√çSTICAS DESCRIPTIVAS (num√©ricas):\")\n",
    "print(df.describe(include=[np.number]).T.round(2))\n",
    "\n",
    "print(\"\\nüìã ESTAD√çSTICAS DESCRIPTIVAS (categ√≥ricas):\")\n",
    "print(df.describe(include=['object', 'category']).T)\n",
    "print(\"\\n‚úÖ An√°lisis exploratorio inicial completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21602d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 5 (mejorada)\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Top atletas por total y pa√≠ses con m√°s registros\n",
    "\n",
    "# 1. Top 5 atletas con mayores totales\n",
    "print(\"üèãÔ∏è‚Äç‚ôÇÔ∏è TOP 5 ATLETAS CON TOTALES M√ÅS ALTOS\\n\" + \"-\"*40)\n",
    "if 'TotalKg' in df.columns and df['TotalKg'].notna().sum() > 0:\n",
    "    top_athletes = df[df['TotalKg'].notna()].nlargest(5, 'TotalKg')[\n",
    "        ['Name', 'Sex', 'Equipment', 'Best3SquatKg', 'Best3BenchKg', 'Best3DeadliftKg', 'TotalKg', 'Country', 'Date']\n",
    "    ]\n",
    "    try:\n",
    "        display(top_athletes)\n",
    "    except Exception:\n",
    "        print(top_athletes.to_string(index=False))\n",
    "else:\n",
    "    print(\"‚ùå No se encontraron registros v√°lidos en 'TotalKg'.\")\n",
    "\n",
    "# 2. Pa√≠ses con m√°s registros\n",
    "print(\"\\nüåç TOP 10 PA√çSES CON M√ÅS REGISTROS\\n\" + \"-\"*40)\n",
    "if 'Country' in df.columns:\n",
    "    country_counts = df['Country'].value_counts(dropna=True).head(10)\n",
    "    print(country_counts)\n",
    "else:\n",
    "    print(\"‚ùå La columna 'Country' no est√° presente en el dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5626d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 6 (optimizada)\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Evaluaci√≥n de calidad y estructura del dataset\n",
    "\n",
    "print(\"üîé AN√ÅLISIS DE CALIDAD DE DATOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"üìÑ Dataset original: {len(df):,} filas\")\n",
    "print(f\"üß© Columnas totales: {len(df.columns)}\\n\")\n",
    "\n",
    "# Columnas clave esperadas\n",
    "key_columns = [\n",
    "    'Name', 'Sex', 'Equipment', 'Age', 'BodyweightKg', 'WeightClassKg',\n",
    "    'Best3SquatKg', 'Best3BenchKg', 'Best3DeadliftKg', 'TotalKg',\n",
    "    'Country', 'Federation', 'Date', 'Dots', 'Wilks'\n",
    "]\n",
    "existing_cols = [col for col in key_columns if col in df.columns]\n",
    "\n",
    "print(\"üìå COLUMNAS PRINCIPALES PRESENTES:\")\n",
    "for col in existing_cols:\n",
    "    print(f\"  - {col}\")\n",
    "print()\n",
    "\n",
    "# Valores faltantes (%)\n",
    "print(\"üö® VALORES FALTANTES (%):\")\n",
    "missing = df[existing_cols].isnull().sum()\n",
    "for col in missing.index:\n",
    "    nulls = missing[col]\n",
    "    pct = (nulls / len(df)) * 100\n",
    "    print(f\"  - {col}: {nulls:,} nulos ({pct:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Fechas\n",
    "if 'Date' in df.columns:\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    min_date, max_date = df['Date'].min(), df['Date'].max()\n",
    "    print(\"üìÜ RANGO DE FECHAS:\")\n",
    "    print(f\"  Desde: {min_date.date() if pd.notna(min_date) else 'N/D'}\")\n",
    "    print(f\"  Hasta: {max_date.date() if pd.notna(max_date) else 'N/D'}\\n\")\n",
    "\n",
    "# TotalKg\n",
    "if 'TotalKg' in df.columns:\n",
    "    print(\"üìä ESTAD√çSTICAS DE 'TotalKg':\")\n",
    "    print(df['TotalKg'].describe(percentiles=[.25, .5, .75]).round(1))\n",
    "    print()\n",
    "\n",
    "# WeightClassKg\n",
    "if 'WeightClassKg' in df.columns:\n",
    "    print(\"‚öñÔ∏è VALORES √öNICOS EN 'WeightClassKg':\")\n",
    "    values = df['WeightClassKg'].dropna().astype(str).unique().tolist()\n",
    "\n",
    "    def parse_wclass(x):\n",
    "        try:\n",
    "            return float(x.replace(\"kg\", \"\").replace(\"+\", \"\").replace(\",\", \".\").strip())\n",
    "        except:\n",
    "            return float('inf')\n",
    "\n",
    "    sorted_values = sorted(values, key=parse_wclass)\n",
    "    print(f\"  Total √∫nicos: {len(sorted_values)}\")\n",
    "    print(\"  Ejemplos:\", sorted_values[:15])\n",
    "\n",
    "    invalid = df[~df['WeightClassKg'].astype(str).str.contains(r'^\\d+(\\.\\d+)?\\+?$', na=False, regex=True)]\n",
    "    if not invalid.empty:\n",
    "        print(f\"\\n‚ö†Ô∏è Valores no est√°ndar detectados en 'WeightClassKg': {len(invalid):,} filas\")\n",
    "else:\n",
    "    print(\"‚ùå No se encontr√≥ la columna 'WeightClassKg'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ccdc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 7 (optimizada)\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# LIMPIEZA Y TRANSFORMACI√ìN DE DATOS\n",
    "\n",
    "print(\"üßπ INICIANDO LIMPIEZA DE DATOS...\")\n",
    "print(f\"üìä Filas iniciales: {len(df):,}\")\n",
    "\n",
    "# 1. Filtrar TotalKg > 0\n",
    "df_clean = df[df['TotalKg'].gt(0)].copy()\n",
    "print(f\"‚úÖ Total > 0: {len(df_clean):,} filas\")\n",
    "\n",
    "# 2. Filtrar fechas desde 1980\n",
    "df_clean = df_clean[df_clean['Date'] >= pd.to_datetime('1980-01-01')]\n",
    "print(f\"‚úÖ Fechas desde 1980: {len(df_clean):,} filas\")\n",
    "\n",
    "# 3. Filtrar totales realistas\n",
    "df_clean = df_clean[df_clean['TotalKg'].between(50, 1500)]\n",
    "print(f\"‚úÖ Totales entre 50 y 1500 kg: {len(df_clean):,} filas\")\n",
    "\n",
    "# 4. Estad√≠sticas por categor√≠a\n",
    "print(\"\\nüìä DISTRIBUCI√ìN CATEG√ìRICA:\")\n",
    "\n",
    "if 'Sex' in df_clean.columns:\n",
    "    print(\"üë§ Sexo:\")\n",
    "    print(df_clean['Sex'].value_counts())\n",
    "\n",
    "if 'Equipment' in df_clean.columns:\n",
    "    print(\"\\nüèãÔ∏è Equipamiento:\")\n",
    "    print(df_clean['Equipment'].value_counts().head(8))\n",
    "\n",
    "# 5. Columnas derivadas\n",
    "print(\"\\nüß† CREANDO COLUMNAS DERIVADAS...\")\n",
    "\n",
    "# Funci√≥n para clasificar peso corporal\n",
    "def classify_weight(bw):\n",
    "    if pd.isna(bw): return \"Desconocido\"\n",
    "    try:\n",
    "        bw = float(bw)\n",
    "        if bw < 59: return \"-59\"\n",
    "        elif bw < 66: return \"59-65\"\n",
    "        elif bw < 74: return \"66-73\"\n",
    "        elif bw < 83: return \"74-82\"\n",
    "        elif bw < 93: return \"83-92\"\n",
    "        elif bw < 105: return \"93-104\"\n",
    "        elif bw < 120: return \"105-119\"\n",
    "        else: return \"120+\"\n",
    "    except:\n",
    "        return \"Desconocido\"\n",
    "\n",
    "# Agregar columnas usando assign (m√°s seguro)\n",
    "df_clean = df_clean.assign(\n",
    "    Year = df_clean['Date'].dt.year,\n",
    "    Decade = (df_clean['Date'].dt.year // 10) * 10,\n",
    "    RelativeStrength = df_clean['TotalKg'] / df_clean['BodyweightKg'].replace({0: np.nan}),\n",
    "    AgeGroup = pd.cut(\n",
    "        df_clean['Age'],\n",
    "        bins=[0, 23, 35, 45, 55, 100],\n",
    "        labels=['Youth', 'Open', 'Masters1', 'Masters2', 'Masters3+'],\n",
    "        include_lowest=True\n",
    "    ),\n",
    "    WeightClass = df_clean['BodyweightKg'].apply(classify_weight)\n",
    ")\n",
    "\n",
    "# Resultado final\n",
    "print(f\"\\nüßº Dataset limpio: {len(df_clean):,} filas\")\n",
    "reduction_pct = ((len(df) - len(df_clean)) / len(df)) * 100\n",
    "print(f\"üìâ Reducci√≥n total: {reduction_pct:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43cdec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 8 (optimizada)\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Normalizaci√≥n de nombres de atletas\n",
    "\n",
    "print(\"üî§ NORMALIZACI√ìN DE NOMBRES DE ATLETAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# -------------------------\n",
    "# Mostrar ejemplos crudos\n",
    "# -------------------------\n",
    "sample_names = df_clean['Name'].dropna().astype(str).unique().tolist()[:30]\n",
    "print(\"üìå EJEMPLOS ORIGINALES:\")\n",
    "for i, name in enumerate(sample_names, 1):\n",
    "    print(f\"{i:2d}. '{name}'\")\n",
    "\n",
    "original_unique_names = df_clean['Name'].nunique()\n",
    "print(f\"\\nüî¢ Nombres √∫nicos (originales): {original_unique_names:,}\")\n",
    "\n",
    "# -------------------------\n",
    "# Funci√≥n de normalizaci√≥n\n",
    "# -------------------------\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def normalize_name(name, remove_accents=False):\n",
    "    \"\"\"Normalizar nombres de atletas para an√°lisis y agrupaci√≥n\"\"\"\n",
    "    if pd.isna(name): return name\n",
    "    name_clean = str(name).strip().title()\n",
    "    name_clean = re.sub(r'\\s+', ' ', name_clean)                             # Espacios m√∫ltiples\n",
    "    name_clean = re.sub(r\"[^\\w\\s\\-\\'√Å√â√ç√ì√ö√ë√°√©√≠√≥√∫√±]\", '', name_clean)          # Eliminar s√≠mbolos raros\n",
    "    if remove_accents:\n",
    "        name_clean = ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', name_clean)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "    return name_clean\n",
    "\n",
    "# -------------------------\n",
    "# Aplicar normalizaci√≥n\n",
    "# -------------------------\n",
    "print(\"\\nüîÅ Aplicando normalizaci√≥n...\")\n",
    "df_clean['NameNormalized'] = df_clean['Name'].apply(normalize_name)\n",
    "\n",
    "normalized_unique_names = df_clean['NameNormalized'].nunique()\n",
    "delta = original_unique_names - normalized_unique_names\n",
    "print(f\"‚úÖ Nombres √∫nicos (normalizados): {normalized_unique_names:,}\")\n",
    "print(f\"üìâ Reducci√≥n de duplicados por formato: {delta:,} nombres unificados\")\n",
    "\n",
    "# -------------------------\n",
    "# Comparar ejemplos\n",
    "# -------------------------\n",
    "print(\"\\nüîç COMPARACI√ìN DE NOMBRES:\")\n",
    "changed = df_clean[['Name', 'NameNormalized']].drop_duplicates()\n",
    "changed = changed[changed['Name'] != changed['NameNormalized']].head(15)\n",
    "\n",
    "if changed.empty:\n",
    "    print(\"No se encontraron diferencias entre nombres originales y normalizados.\")\n",
    "else:\n",
    "    for _, row in changed.iterrows():\n",
    "        print(f\"'{row['Name']}' ‚Üí '{row['NameNormalized']}'\")\n",
    "\n",
    "# -------------------------\n",
    "# Atletas con m√°s competencias\n",
    "# -------------------------\n",
    "print(\"\\nüèãÔ∏è TOP 10 ATLETAS CON M√ÅS COMPETENCIAS:\")\n",
    "athlete_counts = df_clean['NameNormalized'].value_counts().head(10)\n",
    "\n",
    "for name, count in athlete_counts.items():\n",
    "    atleta_df = df_clean[df_clean['NameNormalized'] == name]\n",
    "    year_min = atleta_df['Year'].min()\n",
    "    year_max = atleta_df['Year'].max()\n",
    "    main_country = atleta_df['Country'].mode()[0] if not atleta_df['Country'].isna().all() else \"Desconocido\"\n",
    "    print(f\"üîπ {name}: {count:,} competencias ({year_min}‚Äì{year_max}) ‚Äì {main_country}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc0487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 9\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Limpieza de pa√≠ses, federaciones y agregado de continentes\n",
    "\n",
    "print(\"üåç NORMALIZACI√ìN DE PA√çSES Y FEDERACIONES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# -------------------------\n",
    "# Normalizar nombres de pa√≠ses\n",
    "# -------------------------\n",
    "print(\"üîÅ Limpiando columna 'Country'...\")\n",
    "df_clean['Country'] = df_clean['Country'].fillna(\"Unknown\").str.strip()\n",
    "\n",
    "# Mostrar los m√°s frecuentes\n",
    "print(\"\\nüåê TOP 10 PA√çSES M√ÅS FRECUENTES:\")\n",
    "print(df_clean['Country'].value_counts().head(10))\n",
    "\n",
    "# -------------------------\n",
    "# Limpiar valores poco frecuentes o sospechosos\n",
    "# -------------------------\n",
    "rare_countries = df_clean['Country'].value_counts()[df_clean['Country'].value_counts() < 5]\n",
    "df_clean['Country'] = df_clean['Country'].apply(lambda x: \"Other\" if x in rare_countries else x)\n",
    "\n",
    "# -------------------------\n",
    "# Normalizar nombre de federaciones\n",
    "# -------------------------\n",
    "print(\"\\nüè¢ TOP 10 FEDERACIONES M√ÅS FRECUENTES:\")\n",
    "df_clean['Federation'] = df_clean['Federation'].fillna(\"Unknown\").str.strip()\n",
    "print(df_clean['Federation'].value_counts().head(10))\n",
    "\n",
    "# -------------------------\n",
    "# Agregar columna 'Continent' desde pa√≠s (con ayuda de pycountry_convert)\n",
    "# -------------------------\n",
    "try:\n",
    "    import pycountry_convert as pc\n",
    "\n",
    "    def country_to_continent(country):\n",
    "        try:\n",
    "            # Conversi√≥n pa√≠s ‚Üí c√≥digo alpha-2 ‚Üí continente\n",
    "            country_code = pc.country_name_to_country_alpha2(country, cn_name_format=\"default\")\n",
    "            continent_code = pc.country_alpha2_to_continent_code(country_code)\n",
    "            continent_map = {\n",
    "                \"AF\": \"√Åfrica\",\n",
    "                \"AS\": \"Asia\",\n",
    "                \"EU\": \"Europa\",\n",
    "                \"NA\": \"Am√©rica del Norte\",\n",
    "                \"OC\": \"Ocean√≠a\",\n",
    "                \"SA\": \"Am√©rica del Sur\",\n",
    "                \"AN\": \"Ant√°rtida\"\n",
    "            }\n",
    "            return continent_map.get(continent_code, \"Desconocido\")\n",
    "        except:\n",
    "            return \"Desconocido\"\n",
    "\n",
    "    print(\"\\nüó∫Ô∏è Asignando continentes...\")\n",
    "    df_clean['Continent'] = df_clean['Country'].apply(country_to_continent)\n",
    "    print(\"‚úÖ Continentes asignados.\")\n",
    "    print(df_clean['Continent'].value_counts())\n",
    "\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è No se encontr√≥ pycountry_convert. Ejecuta: pip install pycountry-convert\")\n",
    "    df_clean['Continent'] = \"Desconocido\"\n",
    "\n",
    "print(f\"\\n‚úÖ Total final de filas: {len(df_clean):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c0dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 10\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# An√°lisis de anomal√≠as en edades y totales\n",
    "\n",
    "print(\"üîé INVESTIGANDO ANOMAL√çAS EN LOS DATOS...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# -------------------------\n",
    "# Ni√±os con totales altos\n",
    "# -------------------------\n",
    "print(\"\\nüö® CASOS SOSPECHOSOS: <15 a√±os con >300 kg\")\n",
    "young_heavy = df_clean[(df_clean['Age'] < 15) & (df_clean['TotalKg'] > 300)]\n",
    "\n",
    "if young_heavy.empty:\n",
    "    print(\"‚úÖ No se encontraron casos extremos en esta categor√≠a.\")\n",
    "else:\n",
    "    print(f\"üîç {len(young_heavy)} casos detectados:\\n\")\n",
    "    for _, row in young_heavy[['NameNormalized', 'Age', 'TotalKg', 'BodyweightKg', 'Country', 'Date', 'Federation']].head(10).iterrows():\n",
    "        print(f\"üîπ {row['NameNormalized']}: {row['Age']} a√±os ‚Äî {row['TotalKg']}kg (peso: {row['BodyweightKg']}kg) ‚Äì {row['Country']}\")\n",
    "\n",
    "# -------------------------\n",
    "# Resumen de distribuci√≥n de edad\n",
    "# -------------------------\n",
    "print(\"\\nüìä DISTRIBUCI√ìN DE EDADES:\")\n",
    "print(df_clean['Age'].describe().round(1))\n",
    "\n",
    "# Casos extremos\n",
    "print(\"\\nüìç CASOS EXTREMOS:\")\n",
    "print(f\"üë∂ Menores de 10 a√±os: {len(df_clean[df_clean['Age'] < 10]):,}\")\n",
    "print(f\"üë¥ Mayores de 80 a√±os: {len(df_clean[df_clean['Age'] > 80]):,}\")\n",
    "\n",
    "# -------------------------\n",
    "# An√°lisis por grupo etario detallado\n",
    "# -------------------------\n",
    "print(\"\\nüìà TOTALES PROMEDIO POR GRUPO DE EDAD:\")\n",
    "age_bins = [0, 12, 18, 25, 35, 45, 55, 65, 100]\n",
    "age_labels = ['<12', '12-17', '18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "df_clean['AgeGroupDetailed'] = pd.cut(df_clean['Age'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "age_analysis = df_clean.groupby('AgeGroupDetailed')['TotalKg'].agg(['count', 'mean', 'std', 'min', 'max']).round(1)\n",
    "print(age_analysis)\n",
    "\n",
    "# -------------------------\n",
    "# Casos realmente extremos (error probable)\n",
    "# -------------------------\n",
    "print(\"\\nüßØ POSIBLES ERRORES (Ej. Ni√±os con +200 kg o Adultos Mayores con +600 kg):\")\n",
    "extreme_cases = df_clean[\n",
    "    ((df_clean['Age'] < 12) & (df_clean['TotalKg'] > 200)) |\n",
    "    ((df_clean['Age'] > 80) & (df_clean['TotalKg'] > 600))\n",
    "]\n",
    "\n",
    "if extreme_cases.empty:\n",
    "    print(\"‚úÖ Sin casos extremos evidentes.\")\n",
    "else:\n",
    "    for _, row in extreme_cases[['NameNormalized', 'Age', 'TotalKg', 'Date', 'Federation']].head(5).iterrows():\n",
    "        print(f\"‚ö†Ô∏è {row['NameNormalized']}: {row['Age']} a√±os ‚Äì {row['TotalKg']}kg ({row['Federation']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f554a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 11\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# An√°lisis por clasificaci√≥n etaria oficial (AgeClass)\n",
    "\n",
    "print(\"üß¨ ANALIZANDO DATOS CON CLASIFICACIONES DE EDAD OFICIALES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# -------------------------\n",
    "# Conteo de categor√≠as\n",
    "# -------------------------\n",
    "print(\"\\nüìö CATEGOR√çAS DE EDAD DISPONIBLES:\")\n",
    "age_class_counts = df_clean['AgeClass'].value_counts()\n",
    "print(age_class_counts.head(15))\n",
    "\n",
    "print(f\"\\n‚úÖ Registros con AgeClass: {df_clean['AgeClass'].notna().sum():,}\")\n",
    "print(f\"‚ùå Registros sin AgeClass: {df_clean['AgeClass'].isna().sum():,}\")\n",
    "\n",
    "# -------------------------\n",
    "# Filtrar registros v√°lidos\n",
    "# -------------------------\n",
    "df_clean_age = df_clean[df_clean['AgeClass'].notna()].copy()\n",
    "print(f\"\\nüì¶ Dataset con AgeClass v√°lido: {len(df_clean_age):,} registros\")\n",
    "\n",
    "# -------------------------\n",
    "# Totales por AgeClass\n",
    "# -------------------------\n",
    "print(\"\\nüìä TOTALES POR CATEGOR√çA DE EDAD:\")\n",
    "age_class_stats = df_clean_age.groupby('AgeClass')['TotalKg'].agg(['count', 'mean', 'std', 'min', 'max']).round(1)\n",
    "age_class_stats = age_class_stats.sort_values('count', ascending=False)\n",
    "print(age_class_stats.head(10))\n",
    "\n",
    "# -------------------------\n",
    "# Totales por AgeClass y Sexo\n",
    "# -------------------------\n",
    "print(\"\\n‚öñÔ∏è TOTALES POR CATEGOR√çA DE EDAD Y SEXO:\")\n",
    "age_sex_stats = df_clean_age.groupby(['AgeClass', 'Sex'])['TotalKg'].agg(['count', 'mean']).round(1)\n",
    "\n",
    "if 'M' in df_clean_age['Sex'].unique():\n",
    "    print(\"\\nüë® CATEGOR√çAS MASCULINAS M√ÅS COMUNES:\")\n",
    "    male_stats = age_sex_stats.xs('M', level='Sex').sort_values('count', ascending=False).head(8)\n",
    "    print(male_stats)\n",
    "\n",
    "if 'F' in df_clean_age['Sex'].unique():\n",
    "    print(\"\\nüë© CATEGOR√çAS FEMENINAS M√ÅS COMUNES:\")\n",
    "    female_stats = age_sex_stats.xs('F', level='Sex').sort_values('count', ascending=False).head(8)\n",
    "    print(female_stats)\n",
    "\n",
    "# -------------------------\n",
    "# Actualizar df_clean principal\n",
    "# -------------------------\n",
    "df_clean = df_clean_age.copy()\n",
    "print(f\"\\nüîÅ Dataset actualizado: {len(df_clean):,} registros con AgeClass\")\n",
    "# CELDA 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af6848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 12\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Eliminaci√≥n de registros con AgeClass vac√≠os o no realistas\n",
    "\n",
    "print(\"üßπ ELIMINANDO CATEGOR√çAS DE EDAD NO REALISTAS...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Estado inicial\n",
    "total_before = len(df_clean)\n",
    "print(f\"üî¢ Registros iniciales: {total_before:,}\")\n",
    "\n",
    "# 1. Eliminar AgeClass == '5-12'\n",
    "df_clean = df_clean[df_clean['AgeClass'] != '5-12']\n",
    "\n",
    "# 2. Eliminar registros sin AgeClass\n",
    "df_clean = df_clean[df_clean['AgeClass'].notna()]\n",
    "\n",
    "# Estado final\n",
    "total_after = len(df_clean)\n",
    "removed = total_before - total_after\n",
    "pct_removed = (removed / total_before) * 100\n",
    "\n",
    "print(f\"\\n‚úÖ Registros despu√©s del filtro: {total_after:,}\")\n",
    "print(f\"‚ùå Eliminados: {removed:,} ({pct_removed:.2f}%)\")\n",
    "\n",
    "# Verificar categor√≠as finales\n",
    "print(f\"\\nüìö CATEGOR√çAS RESTANTES:\")\n",
    "remaining_categories = df_clean['AgeClass'].value_counts().sort_index()\n",
    "for cat, count in remaining_categories.items():\n",
    "    avg_total = df_clean[df_clean['AgeClass'] == cat]['TotalKg'].mean()\n",
    "    print(f\"  {cat}: {count:,} registros ‚Äì Promedio TotalKg: {avg_total:.1f}kg\")\n",
    "\n",
    "# Estad√≠sticas de TotalKg\n",
    "print(f\"\\nüìä ESTAD√çSTICAS DE TOTALES (POST-FILTRO):\")\n",
    "print(df_clean['TotalKg'].describe().round(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe5f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 13\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Muestra aleatoria de 20 filas con todas las columnas\n",
    "\n",
    "print(\"üîç MUESTRA ALEATORIA DEL DATASET LIMPIO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verificaci√≥n de columnas\n",
    "print(f\"üß© Total columnas: {len(df_clean.columns)}\")\n",
    "print(\"üìù Columnas disponibles:\")\n",
    "for col in df_clean.columns:\n",
    "    print(f\"  - {col}\")\n",
    "print()\n",
    "\n",
    "# Mostrar 20 filas aleatorias\n",
    "print(\"üìã Muestra aleatoria de 20 filas:\")\n",
    "sample_df = df_clean.sample(n=20, random_state=42)  # Fijamos seed para reproducibilidad\n",
    "\n",
    "try:\n",
    "    display(sample_df)  # Solo si est√°s en Jupyter o Streamlit\n",
    "except NameError:\n",
    "    print(sample_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe64b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 14\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Limpieza avanzada y estandarizaci√≥n de campos\n",
    "\n",
    "print(\"üßº LIMPIEZA AVANZADA DE DATOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Normalizar campos categ√≥ricos\n",
    "# ------------------------------\n",
    "print(\"üî† Normalizando columnas categ√≥ricas...\")\n",
    "\n",
    "df_clean['Sex'] = df_clean['Sex'].str.upper().str.strip()\n",
    "df_clean['Equipment'] = df_clean['Equipment'].str.title().str.strip()\n",
    "df_clean['Tested'] = df_clean['Tested'].astype(str).str.upper().str.strip()\n",
    "df_clean['Federation'] = df_clean['Federation'].astype(str).str.strip()\n",
    "df_clean['Event'] = df_clean['Event'].astype(str).str.title().str.strip()\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Corregir eventos inconsistentes\n",
    "# ------------------------------\n",
    "print(\"üéØ Corrigiendo valores en 'Event'...\")\n",
    "\n",
    "event_map = {\n",
    "    'Sbd': 'Full Power',\n",
    "    'Fullpower': 'Full Power',\n",
    "    'Full Powerlifting': 'Full Power',\n",
    "    'Bench': 'Bench Only',\n",
    "    'Deadlift': 'Deadlift Only',\n",
    "    'Push Pull': 'Push-Pull',\n",
    "    'Pushpull': 'Push-Pull',\n",
    "}\n",
    "df_clean['Event'] = df_clean['Event'].replace(event_map)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Limpiar federaciones nulas o inv√°lidas\n",
    "# ------------------------------\n",
    "print(\"üèõÔ∏è Normalizando federaciones...\")\n",
    "\n",
    "df_clean['Federation'] = df_clean['Federation'].replace(\n",
    "    ['', '-', 'N/A', 'na', 'NaN', 'None'], 'Desconocida'\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Detectar fechas en el futuro\n",
    "# ------------------------------\n",
    "print(\"üïì Verificando fechas futuras...\")\n",
    "\n",
    "today = pd.Timestamp.now()\n",
    "future_dates = df_clean[df_clean['Date'] > today]\n",
    "print(f\"üîÆ Registros con fecha en el futuro: {len(future_dates)}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Eliminar duplicados exactos\n",
    "# ------------------------------\n",
    "print(\"üìé Eliminando duplicados exactos por Nombre + Fecha + Federaci√≥n...\")\n",
    "\n",
    "before_dupes = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates(subset=['NameNormalized', 'Date', 'Federation'])\n",
    "after_dupes = len(df_clean)\n",
    "print(f\"üóëÔ∏è Registros eliminados por duplicados: {before_dupes - after_dupes:,}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Normalizar pa√≠ses (opcional manual)\n",
    "# ------------------------------\n",
    "print(\"üåç Normalizando nombres de pa√≠ses...\")\n",
    "\n",
    "country_map = {\n",
    "    'USA': 'United States',\n",
    "    'UK': 'United Kingdom',\n",
    "    'Korea': 'South Korea',\n",
    "    'UAE': 'United Arab Emirates',\n",
    "    'PR': 'Puerto Rico',\n",
    "    'TR': 'Turkey',\n",
    "    'IR': 'Iran',\n",
    "    # Agrega m√°s seg√∫n lo que detectes\n",
    "}\n",
    "df_clean['Country'] = df_clean['Country'].replace(country_map)\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Crear clasificaci√≥n Raw vs Equipado simplificada\n",
    "# ------------------------------\n",
    "print(\"üèãÔ∏è Clasificando Raw vs Equipado...\")\n",
    "\n",
    "def classify_equipment(e):\n",
    "    if pd.isna(e): return \"Desconocido\"\n",
    "    e = str(e).lower()\n",
    "    if \"raw\" in e: return \"Raw\"\n",
    "    if \"wraps\" in e: return \"Raw\"\n",
    "    if \"single\" in e or \"multi\" in e or \"equipped\" in e: return \"Equipped\"\n",
    "    return \"Otro\"\n",
    "\n",
    "df_clean['RawOrEquipped'] = df_clean['Equipment'].apply(classify_equipment)\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Validaci√≥n final de columnas\n",
    "# ------------------------------\n",
    "print(\"\\n‚úÖ LIMPIEZA AVANZADA COMPLETADA\")\n",
    "print(f\"üß© Columnas disponibles: {len(df_clean.columns)}\")\n",
    "print(\"üîé Nuevas columnas agregadas: ['RawOrEquipped']\")\n",
    "\n",
    "# Opcional: ver una muestra final\n",
    "print(\"\\nüìã Muestra final tras limpieza:\")\n",
    "print(df_clean.sample(5)[['NameNormalized', 'Sex', 'Equipment', 'RawOrEquipped', 'Country', 'Event', 'Federation']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f9b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 15\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Normalizaci√≥n y modelo estrella (dimensiones + hechos)\n",
    "\n",
    "print(\"üîÅ NORMALIZANDO Y CREANDO MODELO ESTRELLA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# -----------------------------\n",
    "# Crear tabla DIM_MEET\n",
    "# -----------------------------\n",
    "print(\"üß± Creando DIM_MEET...\")\n",
    "dim_meet = df_clean[['MeetName', 'MeetTown', 'MeetState', 'MeetCountry', 'Date']].drop_duplicates().reset_index(drop=True)\n",
    "dim_meet['MeetID'] = range(1, len(dim_meet) + 1)\n",
    "print(f\"Dimensi√≥n MEET: {len(dim_meet):,} filas\")\n",
    "\n",
    "# -----------------------------\n",
    "# Crear tabla DIM_FEDERATION\n",
    "# -----------------------------\n",
    "print(\"üß± Creando DIM_FEDERATION...\")\n",
    "dim_fed = df_clean[['Federation', 'ParentFederation']].drop_duplicates().reset_index(drop=True)\n",
    "dim_fed['FederationID'] = range(1, len(dim_fed) + 1)\n",
    "print(f\"Dimensi√≥n FEDERATION: {len(dim_fed):,} filas\")\n",
    "\n",
    "# -----------------------------\n",
    "# Crear tabla DIM_ATLETA\n",
    "# -----------------------------\n",
    "print(\"üß± Creando DIM_ATLETA...\")\n",
    "dim_atleta = df_clean[['NameNormalized', 'Sex', 'Age', 'AgeClass', 'AgeGroup', 'AgeGroupDetailed', 'BodyweightKg', 'WeightClass']].drop_duplicates().reset_index(drop=True)\n",
    "dim_atleta['AthleteID'] = range(1, len(dim_atleta) + 1)\n",
    "print(f\"Dimensi√≥n ATLETA: {len(dim_atleta):,} filas\")\n",
    "\n",
    "# -----------------------------\n",
    "# Reemplazar en tabla de hechos (df_fact)\n",
    "# -----------------------------\n",
    "print(\"üì¶ Construyendo tabla de hechos...\")\n",
    "\n",
    "# Merge federaciones\n",
    "df_fact = df_clean.merge(dim_fed, on=['Federation', 'ParentFederation'], how='left')\n",
    "\n",
    "# Merge meet\n",
    "df_fact = df_fact.merge(dim_meet, on=['MeetName', 'MeetTown', 'MeetState', 'MeetCountry', 'Date'], how='left')\n",
    "\n",
    "# Merge atleta\n",
    "df_fact = df_fact.merge(dim_atleta, on=['NameNormalized', 'Sex', 'Age', 'AgeClass', 'AgeGroup', 'AgeGroupDetailed', 'BodyweightKg', 'WeightClass'], how='left')\n",
    "\n",
    "# Seleccionar columnas finales\n",
    "fact_cols = [\n",
    "    'AthleteID', 'FederationID', 'MeetID',\n",
    "    'Event', 'Equipment', 'Tested',\n",
    "    'Best3SquatKg', 'Best3BenchKg', 'Best3DeadliftKg', 'TotalKg',\n",
    "    'Dots', 'Wilks', 'Glossbrenner', 'Goodlift',\n",
    "    'Year', 'Decade', 'RelativeStrength', 'Place'\n",
    "]\n",
    "\n",
    "df_fact = df_fact[fact_cols].copy()\n",
    "\n",
    "# Mostrar estructura\n",
    "print(f\"\\nüìà TABLA DE HECHOS: {len(df_fact):,} filas √ó {len(df_fact.columns)} columnas\")\n",
    "print(\"üß± Listo para exportar a modelo estrella en GCP BigQuery o Snowflake üöÄ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9b732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 16 - Configuraci√≥n segura para carga a BigQuery (.env en ra√≠z del proyecto)\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# --------------------------\n",
    "# Ruta base del proyecto\n",
    "# --------------------------\n",
    "BASE_DIR = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "\n",
    "# --------------------------\n",
    "# Cargar variables desde el archivo .env en la ra√≠z del proyecto\n",
    "# --------------------------\n",
    "load_dotenv(dotenv_path=BASE_DIR / \".env\")\n",
    "\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "DATASET_ID = os.getenv(\"DATASET_ID\")\n",
    "CREDENTIALS_PATH = os.getenv(\"CREDENTIALS_PATH\")  # Aseg√∫rate de usar esta clave en el .env\n",
    "\n",
    "# --------------------------\n",
    "# Crear cliente BigQuery\n",
    "# --------------------------\n",
    "if not all([PROJECT_ID, DATASET_ID, CREDENTIALS_PATH]):\n",
    "    raise ValueError(\"‚ùå Faltan variables de entorno. Verifica tu archivo .env en la ra√≠z del proyecto.\")\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(BASE_DIR / CREDENTIALS_PATH)\n",
    "client = bigquery.Client(credentials=credentials, project=PROJECT_ID)\n",
    "\n",
    "print(f\"‚úÖ Cliente BigQuery creado con proyecto: {PROJECT_ID}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 17 - Cargar df_clean a BigQuery\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# --------------------------\n",
    "# Verificar DataFrame\n",
    "# --------------------------\n",
    "if 'df_clean' not in locals():\n",
    "    raise ValueError(\"‚ùå df_clean no est√° definido. Aseg√∫rate de haber ejecutado el procesamiento anterior.\")\n",
    "\n",
    "# --------------------------\n",
    "# Nombre completo de la tabla\n",
    "# --------------------------\n",
    "TABLE_ID = f\"{PROJECT_ID}.{DATASET_ID}.results_clean\"\n",
    "\n",
    "print(f\"üì¶ Preparando carga a tabla: {TABLE_ID}\")\n",
    "print(f\"üî¢ Filas: {len(df_clean):,} | Columnas: {len(df_clean.columns)}\")\n",
    "\n",
    "# --------------------------\n",
    "# Cargar a BigQuery\n",
    "# --------------------------\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "    autodetect=True\n",
    ")\n",
    "\n",
    "job = client.load_table_from_dataframe(df_clean, TABLE_ID, job_config=job_config)\n",
    "job.result()  # Esperar a que termine\n",
    "\n",
    "print(\"‚úÖ Carga completada con √©xito\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
