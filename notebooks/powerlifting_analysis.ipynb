{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b4f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 1\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Análisis completo de datos de OpenPowerlifting\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# (Opcional) Carga remota a BigQuery\n",
    "USE_BIGQUERY = True\n",
    "\n",
    "if USE_BIGQUERY:\n",
    "    from google.cloud import bigquery\n",
    "    from google.oauth2 import service_account\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup directorios\n",
    "BASE_DIR = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "# Crear directorios si no existen\n",
    "for d in [DATA_DIR, RAW_DIR, PROCESSED_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"📂 Base directory: {BASE_DIR}\")\n",
    "print(\"✅ Configuración lista para análisis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b8039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 2 (versión mejorada)\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Descarga y conversión eficiente a Parquet desde OpenPowerlifting\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Rutas\n",
    "RAW_DIR = Path(\"data/raw\")\n",
    "PROCESSED_DIR = Path(\"data/processed\")\n",
    "ZIP_URL = \"https://openpowerlifting.gitlab.io/opl-csv/files/openpowerlifting-latest.zip\"\n",
    "ZIP_PATH = RAW_DIR / \"openpowerlifting-latest.zip\"\n",
    "EXTRACT_PATH = RAW_DIR / \"extracted\"\n",
    "PARQUET_PATH = PROCESSED_DIR / \"powerlifting_raw.parquet\"\n",
    "\n",
    "# Crear carpetas necesarias\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EXTRACT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def download_and_convert():\n",
    "    \"\"\"Descargar ZIP, extraer CSV y convertir a Parquet.\"\"\"\n",
    "    \n",
    "    if PARQUET_PATH.exists():\n",
    "        print(f\"📦 Archivo Parquet ya existe: {PARQUET_PATH.name} ({round(os.path.getsize(PARQUET_PATH)/1e6, 1)} MB)\")\n",
    "        return PARQUET_PATH\n",
    "\n",
    "    if not ZIP_PATH.exists():\n",
    "        print(\"📥 Descargando datos de OpenPowerlifting...\")\n",
    "        r = requests.get(ZIP_URL, stream=True)\n",
    "        total = int(r.headers.get('content-length', 0))\n",
    "        with open(ZIP_PATH, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(\"✅ Descarga completada.\")\n",
    "    \n",
    "    print(\"🗂️ Extrayendo archivo ZIP...\")\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(EXTRACT_PATH)\n",
    "    \n",
    "    print(\"🔍 Buscando archivo CSV...\")\n",
    "    csv_files = list(EXTRACT_PATH.glob(\"**/*.csv\"))\n",
    "    main_csv = [f for f in csv_files if 'openpowerlifting' in f.name and f.suffix == '.csv']\n",
    "    \n",
    "    if not main_csv:\n",
    "        raise FileNotFoundError(\"❌ No se encontró el archivo CSV principal.\")\n",
    "    \n",
    "    csv_path = main_csv[0]\n",
    "    print(f\"📄 CSV detectado: {csv_path.name}\")\n",
    "    \n",
    "    print(\"⚙️ Cargando CSV y convirtiendo a Parquet...\")\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "    \n",
    "    print(f\"✅ Datos cargados: {len(df):,} filas. Guardando como Parquet...\")\n",
    "    df.to_parquet(PARQUET_PATH, index=False)\n",
    "    print(f\"🎉 Parquet guardado en: {PARQUET_PATH} ({round(os.path.getsize(PARQUET_PATH)/1e6, 1)} MB)\")\n",
    "\n",
    "    return PARQUET_PATH\n",
    "\n",
    "# Ejecutar\n",
    "parquet_file = download_and_convert()\n",
    "print(f\"📦 Archivo Parquet listo: {parquet_file.name} ({round(os.path.getsize(parquet_file)/1e6, 1)} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd79ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 3 (mejorada)\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Carga de datos desde archivo Parquet\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo Parquet ya procesado\n",
    "parquet_path = Path(\"data/processed/powerlifting_raw.parquet\")\n",
    "\n",
    "# Validar existencia\n",
    "if not parquet_path.exists():\n",
    "    raise FileNotFoundError(\"❌ Archivo Parquet no encontrado. Ejecuta la Celda 2 para generarlo.\")\n",
    "\n",
    "# Cargar datos en memoria eficiente\n",
    "print(\"📂 Cargando datos desde archivo Parquet optimizado...\")\n",
    "df = pd.read_parquet(parquet_path)\n",
    "print(f\"✅ Datos cargados correctamente:\")\n",
    "print(f\"   - Filas:     {df.shape[0]:,}\")\n",
    "print(f\"   - Columnas:  {df.shape[1]:,}\")\n",
    "print(f\"   - Columnas disponibles: {', '.join(df.columns[:8])}... (+{len(df.columns)-8} más)\" if len(df.columns) > 8 else f\"   - Columnas: {df.columns.tolist()}\")\n",
    "print(f\"   - Memoria usada: {df.memory_usage(deep=True).sum() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf353c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 4 (mejorada)\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Análisis exploratorio inicial\n",
    "\n",
    "print(\"📊 INFORMACIÓN BÁSICA DEL DATASET\\n\" + \"-\"*40)\n",
    "print(f\"✔️ Filas         : {df.shape[0]:,}\")\n",
    "print(f\"✔️ Columnas      : {df.shape[1]:,}\")\n",
    "\n",
    "# Vista preliminar de datos\n",
    "print(\"\\n🔍 PRIMERAS 3 FILAS:\")\n",
    "try:\n",
    "    display(df.head(3))  # Jupyter o Streamlit\n",
    "except Exception:\n",
    "    print(df.head(3).to_string(index=False))\n",
    "\n",
    "# Tipos de datos\n",
    "print(\"\\n🧠 TIPOS DE DATOS:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(\"\\nTipos específicos:\")\n",
    "print(df.dtypes.sort_values().astype(str).to_string())\n",
    "\n",
    "# Valores faltantes\n",
    "print(\"\\n⚠️ VALORES FALTANTES (TOP 10):\")\n",
    "missing = df.isna().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "if not missing.empty:\n",
    "    print(missing.head(10))\n",
    "else:\n",
    "    print(\"✅ No hay valores faltantes significativos.\")\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "print(\"\\n📈 ESTADÍSTICAS DESCRIPTIVAS (numéricas):\")\n",
    "print(df.describe(include=[np.number]).T.round(2))\n",
    "\n",
    "print(\"\\n📋 ESTADÍSTICAS DESCRIPTIVAS (categóricas):\")\n",
    "print(df.describe(include=['object', 'category']).T)\n",
    "print(\"\\n✅ Análisis exploratorio inicial completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21602d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 5 (mejorada)\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Top atletas por total y países con más registros\n",
    "\n",
    "# 1. Top 5 atletas con mayores totales\n",
    "print(\"🏋️‍♂️ TOP 5 ATLETAS CON TOTALES MÁS ALTOS\\n\" + \"-\"*40)\n",
    "if 'TotalKg' in df.columns and df['TotalKg'].notna().sum() > 0:\n",
    "    top_athletes = df[df['TotalKg'].notna()].nlargest(5, 'TotalKg')[\n",
    "        ['Name', 'Sex', 'Equipment', 'Best3SquatKg', 'Best3BenchKg', 'Best3DeadliftKg', 'TotalKg', 'Country', 'Date']\n",
    "    ]\n",
    "    try:\n",
    "        display(top_athletes)\n",
    "    except Exception:\n",
    "        print(top_athletes.to_string(index=False))\n",
    "else:\n",
    "    print(\"❌ No se encontraron registros válidos en 'TotalKg'.\")\n",
    "\n",
    "# 2. Países con más registros\n",
    "print(\"\\n🌍 TOP 10 PAÍSES CON MÁS REGISTROS\\n\" + \"-\"*40)\n",
    "if 'Country' in df.columns:\n",
    "    country_counts = df['Country'].value_counts(dropna=True).head(10)\n",
    "    print(country_counts)\n",
    "else:\n",
    "    print(\"❌ La columna 'Country' no está presente en el dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5626d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 6 (optimizada)\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Evaluación de calidad y estructura del dataset\n",
    "\n",
    "print(\"🔎 ANÁLISIS DE CALIDAD DE DATOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"📄 Dataset original: {len(df):,} filas\")\n",
    "print(f\"🧩 Columnas totales: {len(df.columns)}\\n\")\n",
    "\n",
    "# Columnas clave esperadas\n",
    "key_columns = [\n",
    "    'Name', 'Sex', 'Equipment', 'Age', 'BodyweightKg', 'WeightClassKg',\n",
    "    'Best3SquatKg', 'Best3BenchKg', 'Best3DeadliftKg', 'TotalKg',\n",
    "    'Country', 'Federation', 'Date', 'Dots', 'Wilks'\n",
    "]\n",
    "existing_cols = [col for col in key_columns if col in df.columns]\n",
    "\n",
    "print(\"📌 COLUMNAS PRINCIPALES PRESENTES:\")\n",
    "for col in existing_cols:\n",
    "    print(f\"  - {col}\")\n",
    "print()\n",
    "\n",
    "# Valores faltantes (%)\n",
    "print(\"🚨 VALORES FALTANTES (%):\")\n",
    "missing = df[existing_cols].isnull().sum()\n",
    "for col in missing.index:\n",
    "    nulls = missing[col]\n",
    "    pct = (nulls / len(df)) * 100\n",
    "    print(f\"  - {col}: {nulls:,} nulos ({pct:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Fechas\n",
    "if 'Date' in df.columns:\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    min_date, max_date = df['Date'].min(), df['Date'].max()\n",
    "    print(\"📆 RANGO DE FECHAS:\")\n",
    "    print(f\"  Desde: {min_date.date() if pd.notna(min_date) else 'N/D'}\")\n",
    "    print(f\"  Hasta: {max_date.date() if pd.notna(max_date) else 'N/D'}\\n\")\n",
    "\n",
    "# TotalKg\n",
    "if 'TotalKg' in df.columns:\n",
    "    print(\"📊 ESTADÍSTICAS DE 'TotalKg':\")\n",
    "    print(df['TotalKg'].describe(percentiles=[.25, .5, .75]).round(1))\n",
    "    print()\n",
    "\n",
    "# WeightClassKg\n",
    "if 'WeightClassKg' in df.columns:\n",
    "    print(\"⚖️ VALORES ÚNICOS EN 'WeightClassKg':\")\n",
    "    values = df['WeightClassKg'].dropna().astype(str).unique().tolist()\n",
    "\n",
    "    def parse_wclass(x):\n",
    "        try:\n",
    "            return float(x.replace(\"kg\", \"\").replace(\"+\", \"\").replace(\",\", \".\").strip())\n",
    "        except:\n",
    "            return float('inf')\n",
    "\n",
    "    sorted_values = sorted(values, key=parse_wclass)\n",
    "    print(f\"  Total únicos: {len(sorted_values)}\")\n",
    "    print(\"  Ejemplos:\", sorted_values[:15])\n",
    "\n",
    "    invalid = df[~df['WeightClassKg'].astype(str).str.contains(r'^\\d+(\\.\\d+)?\\+?$', na=False, regex=True)]\n",
    "    if not invalid.empty:\n",
    "        print(f\"\\n⚠️ Valores no estándar detectados en 'WeightClassKg': {len(invalid):,} filas\")\n",
    "else:\n",
    "    print(\"❌ No se encontró la columna 'WeightClassKg'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ccdc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 7 (optimizada)\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# LIMPIEZA Y TRANSFORMACIÓN DE DATOS\n",
    "\n",
    "print(\"🧹 INICIANDO LIMPIEZA DE DATOS...\")\n",
    "print(f\"📊 Filas iniciales: {len(df):,}\")\n",
    "\n",
    "# 1. Filtrar TotalKg > 0\n",
    "df_clean = df[df['TotalKg'].gt(0)].copy()\n",
    "print(f\"✅ Total > 0: {len(df_clean):,} filas\")\n",
    "\n",
    "# 2. Filtrar fechas desde 1980\n",
    "df_clean = df_clean[df_clean['Date'] >= pd.to_datetime('1980-01-01')]\n",
    "print(f\"✅ Fechas desde 1980: {len(df_clean):,} filas\")\n",
    "\n",
    "# 3. Filtrar totales realistas\n",
    "df_clean = df_clean[df_clean['TotalKg'].between(50, 1500)]\n",
    "print(f\"✅ Totales entre 50 y 1500 kg: {len(df_clean):,} filas\")\n",
    "\n",
    "# 4. Estadísticas por categoría\n",
    "print(\"\\n📊 DISTRIBUCIÓN CATEGÓRICA:\")\n",
    "\n",
    "if 'Sex' in df_clean.columns:\n",
    "    print(\"👤 Sexo:\")\n",
    "    print(df_clean['Sex'].value_counts())\n",
    "\n",
    "if 'Equipment' in df_clean.columns:\n",
    "    print(\"\\n🏋️ Equipamiento:\")\n",
    "    print(df_clean['Equipment'].value_counts().head(8))\n",
    "\n",
    "# 5. Columnas derivadas\n",
    "print(\"\\n🧠 CREANDO COLUMNAS DERIVADAS...\")\n",
    "\n",
    "# Función para clasificar peso corporal\n",
    "def classify_weight(bw):\n",
    "    if pd.isna(bw): return \"Desconocido\"\n",
    "    try:\n",
    "        bw = float(bw)\n",
    "        if bw < 59: return \"-59\"\n",
    "        elif bw < 66: return \"59-65\"\n",
    "        elif bw < 74: return \"66-73\"\n",
    "        elif bw < 83: return \"74-82\"\n",
    "        elif bw < 93: return \"83-92\"\n",
    "        elif bw < 105: return \"93-104\"\n",
    "        elif bw < 120: return \"105-119\"\n",
    "        else: return \"120+\"\n",
    "    except:\n",
    "        return \"Desconocido\"\n",
    "\n",
    "# Agregar columnas usando assign (más seguro)\n",
    "df_clean = df_clean.assign(\n",
    "    Year = df_clean['Date'].dt.year,\n",
    "    Decade = (df_clean['Date'].dt.year // 10) * 10,\n",
    "    RelativeStrength = df_clean['TotalKg'] / df_clean['BodyweightKg'].replace({0: np.nan}),\n",
    "    AgeGroup = pd.cut(\n",
    "        df_clean['Age'],\n",
    "        bins=[0, 23, 35, 45, 55, 100],\n",
    "        labels=['Youth', 'Open', 'Masters1', 'Masters2', 'Masters3+'],\n",
    "        include_lowest=True\n",
    "    ),\n",
    "    WeightClass = df_clean['BodyweightKg'].apply(classify_weight)\n",
    ")\n",
    "\n",
    "# Resultado final\n",
    "print(f\"\\n🧼 Dataset limpio: {len(df_clean):,} filas\")\n",
    "reduction_pct = ((len(df) - len(df_clean)) / len(df)) * 100\n",
    "print(f\"📉 Reducción total: {reduction_pct:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43cdec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 8 (optimizada)\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Normalización de nombres de atletas\n",
    "\n",
    "print(\"🔤 NORMALIZACIÓN DE NOMBRES DE ATLETAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# -------------------------\n",
    "# Mostrar ejemplos crudos\n",
    "# -------------------------\n",
    "sample_names = df_clean['Name'].dropna().astype(str).unique().tolist()[:30]\n",
    "print(\"📌 EJEMPLOS ORIGINALES:\")\n",
    "for i, name in enumerate(sample_names, 1):\n",
    "    print(f\"{i:2d}. '{name}'\")\n",
    "\n",
    "original_unique_names = df_clean['Name'].nunique()\n",
    "print(f\"\\n🔢 Nombres únicos (originales): {original_unique_names:,}\")\n",
    "\n",
    "# -------------------------\n",
    "# Función de normalización\n",
    "# -------------------------\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def normalize_name(name, remove_accents=False):\n",
    "    \"\"\"Normalizar nombres de atletas para análisis y agrupación\"\"\"\n",
    "    if pd.isna(name): return name\n",
    "    name_clean = str(name).strip().title()\n",
    "    name_clean = re.sub(r'\\s+', ' ', name_clean)                             # Espacios múltiples\n",
    "    name_clean = re.sub(r\"[^\\w\\s\\-\\'ÁÉÍÓÚÑáéíóúñ]\", '', name_clean)          # Eliminar símbolos raros\n",
    "    if remove_accents:\n",
    "        name_clean = ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', name_clean)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "    return name_clean\n",
    "\n",
    "# -------------------------\n",
    "# Aplicar normalización\n",
    "# -------------------------\n",
    "print(\"\\n🔁 Aplicando normalización...\")\n",
    "df_clean['NameNormalized'] = df_clean['Name'].apply(normalize_name)\n",
    "\n",
    "normalized_unique_names = df_clean['NameNormalized'].nunique()\n",
    "delta = original_unique_names - normalized_unique_names\n",
    "print(f\"✅ Nombres únicos (normalizados): {normalized_unique_names:,}\")\n",
    "print(f\"📉 Reducción de duplicados por formato: {delta:,} nombres unificados\")\n",
    "\n",
    "# -------------------------\n",
    "# Comparar ejemplos\n",
    "# -------------------------\n",
    "print(\"\\n🔍 COMPARACIÓN DE NOMBRES:\")\n",
    "changed = df_clean[['Name', 'NameNormalized']].drop_duplicates()\n",
    "changed = changed[changed['Name'] != changed['NameNormalized']].head(15)\n",
    "\n",
    "if changed.empty:\n",
    "    print(\"No se encontraron diferencias entre nombres originales y normalizados.\")\n",
    "else:\n",
    "    for _, row in changed.iterrows():\n",
    "        print(f\"'{row['Name']}' → '{row['NameNormalized']}'\")\n",
    "\n",
    "# -------------------------\n",
    "# Atletas con más competencias\n",
    "# -------------------------\n",
    "print(\"\\n🏋️ TOP 10 ATLETAS CON MÁS COMPETENCIAS:\")\n",
    "athlete_counts = df_clean['NameNormalized'].value_counts().head(10)\n",
    "\n",
    "for name, count in athlete_counts.items():\n",
    "    atleta_df = df_clean[df_clean['NameNormalized'] == name]\n",
    "    year_min = atleta_df['Year'].min()\n",
    "    year_max = atleta_df['Year'].max()\n",
    "    main_country = atleta_df['Country'].mode()[0] if not atleta_df['Country'].isna().all() else \"Desconocido\"\n",
    "    print(f\"🔹 {name}: {count:,} competencias ({year_min}–{year_max}) – {main_country}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc0487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 9\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Limpieza de países, federaciones y agregado de continentes\n",
    "\n",
    "print(\"🌍 NORMALIZACIÓN DE PAÍSES Y FEDERACIONES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# -------------------------\n",
    "# Normalizar nombres de países\n",
    "# -------------------------\n",
    "print(\"🔁 Limpiando columna 'Country'...\")\n",
    "df_clean['Country'] = df_clean['Country'].fillna(\"Unknown\").str.strip()\n",
    "\n",
    "# Mostrar los más frecuentes\n",
    "print(\"\\n🌐 TOP 10 PAÍSES MÁS FRECUENTES:\")\n",
    "print(df_clean['Country'].value_counts().head(10))\n",
    "\n",
    "# -------------------------\n",
    "# Limpiar valores poco frecuentes o sospechosos\n",
    "# -------------------------\n",
    "rare_countries = df_clean['Country'].value_counts()[df_clean['Country'].value_counts() < 5]\n",
    "df_clean['Country'] = df_clean['Country'].apply(lambda x: \"Other\" if x in rare_countries else x)\n",
    "\n",
    "# -------------------------\n",
    "# Normalizar nombre de federaciones\n",
    "# -------------------------\n",
    "print(\"\\n🏢 TOP 10 FEDERACIONES MÁS FRECUENTES:\")\n",
    "df_clean['Federation'] = df_clean['Federation'].fillna(\"Unknown\").str.strip()\n",
    "print(df_clean['Federation'].value_counts().head(10))\n",
    "\n",
    "# -------------------------\n",
    "# Agregar columna 'Continent' desde país (con ayuda de pycountry_convert)\n",
    "# -------------------------\n",
    "try:\n",
    "    import pycountry_convert as pc\n",
    "\n",
    "    def country_to_continent(country):\n",
    "        try:\n",
    "            # Conversión país → código alpha-2 → continente\n",
    "            country_code = pc.country_name_to_country_alpha2(country, cn_name_format=\"default\")\n",
    "            continent_code = pc.country_alpha2_to_continent_code(country_code)\n",
    "            continent_map = {\n",
    "                \"AF\": \"África\",\n",
    "                \"AS\": \"Asia\",\n",
    "                \"EU\": \"Europa\",\n",
    "                \"NA\": \"América del Norte\",\n",
    "                \"OC\": \"Oceanía\",\n",
    "                \"SA\": \"América del Sur\",\n",
    "                \"AN\": \"Antártida\"\n",
    "            }\n",
    "            return continent_map.get(continent_code, \"Desconocido\")\n",
    "        except:\n",
    "            return \"Desconocido\"\n",
    "\n",
    "    print(\"\\n🗺️ Asignando continentes...\")\n",
    "    df_clean['Continent'] = df_clean['Country'].apply(country_to_continent)\n",
    "    print(\"✅ Continentes asignados.\")\n",
    "    print(df_clean['Continent'].value_counts())\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⚠️ No se encontró pycountry_convert. Ejecuta: pip install pycountry-convert\")\n",
    "    df_clean['Continent'] = \"Desconocido\"\n",
    "\n",
    "print(f\"\\n✅ Total final de filas: {len(df_clean):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c0dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 10\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Análisis de anomalías en edades y totales\n",
    "\n",
    "print(\"🔎 INVESTIGANDO ANOMALÍAS EN LOS DATOS...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# -------------------------\n",
    "# Niños con totales altos\n",
    "# -------------------------\n",
    "print(\"\\n🚨 CASOS SOSPECHOSOS: <15 años con >300 kg\")\n",
    "young_heavy = df_clean[(df_clean['Age'] < 15) & (df_clean['TotalKg'] > 300)]\n",
    "\n",
    "if young_heavy.empty:\n",
    "    print(\"✅ No se encontraron casos extremos en esta categoría.\")\n",
    "else:\n",
    "    print(f\"🔍 {len(young_heavy)} casos detectados:\\n\")\n",
    "    for _, row in young_heavy[['NameNormalized', 'Age', 'TotalKg', 'BodyweightKg', 'Country', 'Date', 'Federation']].head(10).iterrows():\n",
    "        print(f\"🔹 {row['NameNormalized']}: {row['Age']} años — {row['TotalKg']}kg (peso: {row['BodyweightKg']}kg) – {row['Country']}\")\n",
    "\n",
    "# -------------------------\n",
    "# Resumen de distribución de edad\n",
    "# -------------------------\n",
    "print(\"\\n📊 DISTRIBUCIÓN DE EDADES:\")\n",
    "print(df_clean['Age'].describe().round(1))\n",
    "\n",
    "# Casos extremos\n",
    "print(\"\\n📍 CASOS EXTREMOS:\")\n",
    "print(f\"👶 Menores de 10 años: {len(df_clean[df_clean['Age'] < 10]):,}\")\n",
    "print(f\"👴 Mayores de 80 años: {len(df_clean[df_clean['Age'] > 80]):,}\")\n",
    "\n",
    "# -------------------------\n",
    "# Análisis por grupo etario detallado\n",
    "# -------------------------\n",
    "print(\"\\n📈 TOTALES PROMEDIO POR GRUPO DE EDAD:\")\n",
    "age_bins = [0, 12, 18, 25, 35, 45, 55, 65, 100]\n",
    "age_labels = ['<12', '12-17', '18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "df_clean['AgeGroupDetailed'] = pd.cut(df_clean['Age'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "age_analysis = df_clean.groupby('AgeGroupDetailed')['TotalKg'].agg(['count', 'mean', 'std', 'min', 'max']).round(1)\n",
    "print(age_analysis)\n",
    "\n",
    "# -------------------------\n",
    "# Casos realmente extremos (error probable)\n",
    "# -------------------------\n",
    "print(\"\\n🧯 POSIBLES ERRORES (Ej. Niños con +200 kg o Adultos Mayores con +600 kg):\")\n",
    "extreme_cases = df_clean[\n",
    "    ((df_clean['Age'] < 12) & (df_clean['TotalKg'] > 200)) |\n",
    "    ((df_clean['Age'] > 80) & (df_clean['TotalKg'] > 600))\n",
    "]\n",
    "\n",
    "if extreme_cases.empty:\n",
    "    print(\"✅ Sin casos extremos evidentes.\")\n",
    "else:\n",
    "    for _, row in extreme_cases[['NameNormalized', 'Age', 'TotalKg', 'Date', 'Federation']].head(5).iterrows():\n",
    "        print(f\"⚠️ {row['NameNormalized']}: {row['Age']} años – {row['TotalKg']}kg ({row['Federation']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f554a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 11\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Análisis por clasificación etaria oficial (AgeClass)\n",
    "\n",
    "print(\"🧬 ANALIZANDO DATOS CON CLASIFICACIONES DE EDAD OFICIALES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# -------------------------\n",
    "# Conteo de categorías\n",
    "# -------------------------\n",
    "print(\"\\n📚 CATEGORÍAS DE EDAD DISPONIBLES:\")\n",
    "age_class_counts = df_clean['AgeClass'].value_counts()\n",
    "print(age_class_counts.head(15))\n",
    "\n",
    "print(f\"\\n✅ Registros con AgeClass: {df_clean['AgeClass'].notna().sum():,}\")\n",
    "print(f\"❌ Registros sin AgeClass: {df_clean['AgeClass'].isna().sum():,}\")\n",
    "\n",
    "# -------------------------\n",
    "# Filtrar registros válidos\n",
    "# -------------------------\n",
    "df_clean_age = df_clean[df_clean['AgeClass'].notna()].copy()\n",
    "print(f\"\\n📦 Dataset con AgeClass válido: {len(df_clean_age):,} registros\")\n",
    "\n",
    "# -------------------------\n",
    "# Totales por AgeClass\n",
    "# -------------------------\n",
    "print(\"\\n📊 TOTALES POR CATEGORÍA DE EDAD:\")\n",
    "age_class_stats = df_clean_age.groupby('AgeClass')['TotalKg'].agg(['count', 'mean', 'std', 'min', 'max']).round(1)\n",
    "age_class_stats = age_class_stats.sort_values('count', ascending=False)\n",
    "print(age_class_stats.head(10))\n",
    "\n",
    "# -------------------------\n",
    "# Totales por AgeClass y Sexo\n",
    "# -------------------------\n",
    "print(\"\\n⚖️ TOTALES POR CATEGORÍA DE EDAD Y SEXO:\")\n",
    "age_sex_stats = df_clean_age.groupby(['AgeClass', 'Sex'])['TotalKg'].agg(['count', 'mean']).round(1)\n",
    "\n",
    "if 'M' in df_clean_age['Sex'].unique():\n",
    "    print(\"\\n👨 CATEGORÍAS MASCULINAS MÁS COMUNES:\")\n",
    "    male_stats = age_sex_stats.xs('M', level='Sex').sort_values('count', ascending=False).head(8)\n",
    "    print(male_stats)\n",
    "\n",
    "if 'F' in df_clean_age['Sex'].unique():\n",
    "    print(\"\\n👩 CATEGORÍAS FEMENINAS MÁS COMUNES:\")\n",
    "    female_stats = age_sex_stats.xs('F', level='Sex').sort_values('count', ascending=False).head(8)\n",
    "    print(female_stats)\n",
    "\n",
    "# -------------------------\n",
    "# Actualizar df_clean principal\n",
    "# -------------------------\n",
    "df_clean = df_clean_age.copy()\n",
    "print(f\"\\n🔁 Dataset actualizado: {len(df_clean):,} registros con AgeClass\")\n",
    "# CELDA 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af6848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 12\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Eliminación de registros con AgeClass vacíos o no realistas\n",
    "\n",
    "print(\"🧹 ELIMINANDO CATEGORÍAS DE EDAD NO REALISTAS...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Estado inicial\n",
    "total_before = len(df_clean)\n",
    "print(f\"🔢 Registros iniciales: {total_before:,}\")\n",
    "\n",
    "# 1. Eliminar AgeClass == '5-12'\n",
    "df_clean = df_clean[df_clean['AgeClass'] != '5-12']\n",
    "\n",
    "# 2. Eliminar registros sin AgeClass\n",
    "df_clean = df_clean[df_clean['AgeClass'].notna()]\n",
    "\n",
    "# Estado final\n",
    "total_after = len(df_clean)\n",
    "removed = total_before - total_after\n",
    "pct_removed = (removed / total_before) * 100\n",
    "\n",
    "print(f\"\\n✅ Registros después del filtro: {total_after:,}\")\n",
    "print(f\"❌ Eliminados: {removed:,} ({pct_removed:.2f}%)\")\n",
    "\n",
    "# Verificar categorías finales\n",
    "print(f\"\\n📚 CATEGORÍAS RESTANTES:\")\n",
    "remaining_categories = df_clean['AgeClass'].value_counts().sort_index()\n",
    "for cat, count in remaining_categories.items():\n",
    "    avg_total = df_clean[df_clean['AgeClass'] == cat]['TotalKg'].mean()\n",
    "    print(f\"  {cat}: {count:,} registros – Promedio TotalKg: {avg_total:.1f}kg\")\n",
    "\n",
    "# Estadísticas de TotalKg\n",
    "print(f\"\\n📊 ESTADÍSTICAS DE TOTALES (POST-FILTRO):\")\n",
    "print(df_clean['TotalKg'].describe().round(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe5f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 13\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Muestra aleatoria de 20 filas con todas las columnas\n",
    "\n",
    "print(\"🔍 MUESTRA ALEATORIA DEL DATASET LIMPIO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verificación de columnas\n",
    "print(f\"🧩 Total columnas: {len(df_clean.columns)}\")\n",
    "print(\"📝 Columnas disponibles:\")\n",
    "for col in df_clean.columns:\n",
    "    print(f\"  - {col}\")\n",
    "print()\n",
    "\n",
    "# Mostrar 20 filas aleatorias\n",
    "print(\"📋 Muestra aleatoria de 20 filas:\")\n",
    "sample_df = df_clean.sample(n=20, random_state=42)  # Fijamos seed para reproducibilidad\n",
    "\n",
    "try:\n",
    "    display(sample_df)  # Solo si estás en Jupyter o Streamlit\n",
    "except NameError:\n",
    "    print(sample_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe64b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 14\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Limpieza avanzada y estandarización de campos\n",
    "\n",
    "print(\"🧼 LIMPIEZA AVANZADA DE DATOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Normalizar campos categóricos\n",
    "# ------------------------------\n",
    "print(\"🔠 Normalizando columnas categóricas...\")\n",
    "\n",
    "df_clean['Sex'] = df_clean['Sex'].str.upper().str.strip()\n",
    "df_clean['Equipment'] = df_clean['Equipment'].str.title().str.strip()\n",
    "df_clean['Tested'] = df_clean['Tested'].astype(str).str.upper().str.strip()\n",
    "df_clean['Federation'] = df_clean['Federation'].astype(str).str.strip()\n",
    "df_clean['Event'] = df_clean['Event'].astype(str).str.title().str.strip()\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Corregir eventos inconsistentes\n",
    "# ------------------------------\n",
    "print(\"🎯 Corrigiendo valores en 'Event'...\")\n",
    "\n",
    "event_map = {\n",
    "    'Sbd': 'Full Power',\n",
    "    'Fullpower': 'Full Power',\n",
    "    'Full Powerlifting': 'Full Power',\n",
    "    'Bench': 'Bench Only',\n",
    "    'Deadlift': 'Deadlift Only',\n",
    "    'Push Pull': 'Push-Pull',\n",
    "    'Pushpull': 'Push-Pull',\n",
    "}\n",
    "df_clean['Event'] = df_clean['Event'].replace(event_map)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Limpiar federaciones nulas o inválidas\n",
    "# ------------------------------\n",
    "print(\"🏛️ Normalizando federaciones...\")\n",
    "\n",
    "df_clean['Federation'] = df_clean['Federation'].replace(\n",
    "    ['', '-', 'N/A', 'na', 'NaN', 'None'], 'Desconocida'\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Detectar fechas en el futuro\n",
    "# ------------------------------\n",
    "print(\"🕓 Verificando fechas futuras...\")\n",
    "\n",
    "today = pd.Timestamp.now()\n",
    "future_dates = df_clean[df_clean['Date'] > today]\n",
    "print(f\"🔮 Registros con fecha en el futuro: {len(future_dates)}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Eliminar duplicados exactos\n",
    "# ------------------------------\n",
    "print(\"📎 Eliminando duplicados exactos por Nombre + Fecha + Federación...\")\n",
    "\n",
    "before_dupes = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates(subset=['NameNormalized', 'Date', 'Federation'])\n",
    "after_dupes = len(df_clean)\n",
    "print(f\"🗑️ Registros eliminados por duplicados: {before_dupes - after_dupes:,}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Normalizar países (opcional manual)\n",
    "# ------------------------------\n",
    "print(\"🌍 Normalizando nombres de países...\")\n",
    "\n",
    "country_map = {\n",
    "    'USA': 'United States',\n",
    "    'UK': 'United Kingdom',\n",
    "    'Korea': 'South Korea',\n",
    "    'UAE': 'United Arab Emirates',\n",
    "    'PR': 'Puerto Rico',\n",
    "    'TR': 'Turkey',\n",
    "    'IR': 'Iran',\n",
    "    # Agrega más según lo que detectes\n",
    "}\n",
    "df_clean['Country'] = df_clean['Country'].replace(country_map)\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Crear clasificación Raw vs Equipado simplificada\n",
    "# ------------------------------\n",
    "print(\"🏋️ Clasificando Raw vs Equipado...\")\n",
    "\n",
    "def classify_equipment(e):\n",
    "    if pd.isna(e): return \"Desconocido\"\n",
    "    e = str(e).lower()\n",
    "    if \"raw\" in e: return \"Raw\"\n",
    "    if \"wraps\" in e: return \"Raw\"\n",
    "    if \"single\" in e or \"multi\" in e or \"equipped\" in e: return \"Equipped\"\n",
    "    return \"Otro\"\n",
    "\n",
    "df_clean['RawOrEquipped'] = df_clean['Equipment'].apply(classify_equipment)\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Validación final de columnas\n",
    "# ------------------------------\n",
    "print(\"\\n✅ LIMPIEZA AVANZADA COMPLETADA\")\n",
    "print(f\"🧩 Columnas disponibles: {len(df_clean.columns)}\")\n",
    "print(\"🔎 Nuevas columnas agregadas: ['RawOrEquipped']\")\n",
    "\n",
    "# Opcional: ver una muestra final\n",
    "print(\"\\n📋 Muestra final tras limpieza:\")\n",
    "print(df_clean.sample(5)[['NameNormalized', 'Sex', 'Equipment', 'RawOrEquipped', 'Country', 'Event', 'Federation']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f9b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 15\n",
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# Powerlifting Data Analysis\n",
    "# Normalización y modelo estrella (dimensiones + hechos)\n",
    "\n",
    "print(\"🔁 NORMALIZANDO Y CREANDO MODELO ESTRELLA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# -----------------------------\n",
    "# Crear tabla DIM_MEET\n",
    "# -----------------------------\n",
    "print(\"🧱 Creando DIM_MEET...\")\n",
    "dim_meet = df_clean[['MeetName', 'MeetTown', 'MeetState', 'MeetCountry', 'Date']].drop_duplicates().reset_index(drop=True)\n",
    "dim_meet['MeetID'] = range(1, len(dim_meet) + 1)\n",
    "print(f\"Dimensión MEET: {len(dim_meet):,} filas\")\n",
    "\n",
    "# -----------------------------\n",
    "# Crear tabla DIM_FEDERATION\n",
    "# -----------------------------\n",
    "print(\"🧱 Creando DIM_FEDERATION...\")\n",
    "dim_fed = df_clean[['Federation', 'ParentFederation']].drop_duplicates().reset_index(drop=True)\n",
    "dim_fed['FederationID'] = range(1, len(dim_fed) + 1)\n",
    "print(f\"Dimensión FEDERATION: {len(dim_fed):,} filas\")\n",
    "\n",
    "# -----------------------------\n",
    "# Crear tabla DIM_ATLETA\n",
    "# -----------------------------\n",
    "print(\"🧱 Creando DIM_ATLETA...\")\n",
    "dim_atleta = df_clean[['NameNormalized', 'Sex', 'Age', 'AgeClass', 'AgeGroup', 'AgeGroupDetailed', 'BodyweightKg', 'WeightClass']].drop_duplicates().reset_index(drop=True)\n",
    "dim_atleta['AthleteID'] = range(1, len(dim_atleta) + 1)\n",
    "print(f\"Dimensión ATLETA: {len(dim_atleta):,} filas\")\n",
    "\n",
    "# -----------------------------\n",
    "# Reemplazar en tabla de hechos (df_fact)\n",
    "# -----------------------------\n",
    "print(\"📦 Construyendo tabla de hechos...\")\n",
    "\n",
    "# Merge federaciones\n",
    "df_fact = df_clean.merge(dim_fed, on=['Federation', 'ParentFederation'], how='left')\n",
    "\n",
    "# Merge meet\n",
    "df_fact = df_fact.merge(dim_meet, on=['MeetName', 'MeetTown', 'MeetState', 'MeetCountry', 'Date'], how='left')\n",
    "\n",
    "# Merge atleta\n",
    "df_fact = df_fact.merge(dim_atleta, on=['NameNormalized', 'Sex', 'Age', 'AgeClass', 'AgeGroup', 'AgeGroupDetailed', 'BodyweightKg', 'WeightClass'], how='left')\n",
    "\n",
    "# Seleccionar columnas finales\n",
    "fact_cols = [\n",
    "    'AthleteID', 'FederationID', 'MeetID',\n",
    "    'Event', 'Equipment', 'Tested',\n",
    "    'Best3SquatKg', 'Best3BenchKg', 'Best3DeadliftKg', 'TotalKg',\n",
    "    'Dots', 'Wilks', 'Glossbrenner', 'Goodlift',\n",
    "    'Year', 'Decade', 'RelativeStrength', 'Place'\n",
    "]\n",
    "\n",
    "df_fact = df_fact[fact_cols].copy()\n",
    "\n",
    "# Mostrar estructura\n",
    "print(f\"\\n📈 TABLA DE HECHOS: {len(df_fact):,} filas × {len(df_fact.columns)} columnas\")\n",
    "print(\"🧱 Listo para exportar a modelo estrella en GCP BigQuery o Snowflake 🚀\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9b732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 16 - Configuración segura para carga a BigQuery (.env en raíz del proyecto)\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# --------------------------\n",
    "# Ruta base del proyecto\n",
    "# --------------------------\n",
    "BASE_DIR = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "\n",
    "# --------------------------\n",
    "# Cargar variables desde el archivo .env en la raíz del proyecto\n",
    "# --------------------------\n",
    "load_dotenv(dotenv_path=BASE_DIR / \".env\")\n",
    "\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "DATASET_ID = os.getenv(\"DATASET_ID\")\n",
    "CREDENTIALS_PATH = os.getenv(\"CREDENTIALS_PATH\")  # Asegúrate de usar esta clave en el .env\n",
    "\n",
    "# --------------------------\n",
    "# Crear cliente BigQuery\n",
    "# --------------------------\n",
    "if not all([PROJECT_ID, DATASET_ID, CREDENTIALS_PATH]):\n",
    "    raise ValueError(\"❌ Faltan variables de entorno. Verifica tu archivo .env en la raíz del proyecto.\")\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(BASE_DIR / CREDENTIALS_PATH)\n",
    "client = bigquery.Client(credentials=credentials, project=PROJECT_ID)\n",
    "\n",
    "print(f\"✅ Cliente BigQuery creado con proyecto: {PROJECT_ID}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 17 - Cargar df_clean a BigQuery\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# --------------------------\n",
    "# Verificar DataFrame\n",
    "# --------------------------\n",
    "if 'df_clean' not in locals():\n",
    "    raise ValueError(\"❌ df_clean no está definido. Asegúrate de haber ejecutado el procesamiento anterior.\")\n",
    "\n",
    "# --------------------------\n",
    "# Nombre completo de la tabla\n",
    "# --------------------------\n",
    "TABLE_ID = f\"{PROJECT_ID}.{DATASET_ID}.results_clean\"\n",
    "\n",
    "print(f\"📦 Preparando carga a tabla: {TABLE_ID}\")\n",
    "print(f\"🔢 Filas: {len(df_clean):,} | Columnas: {len(df_clean.columns)}\")\n",
    "\n",
    "# --------------------------\n",
    "# Cargar a BigQuery\n",
    "# --------------------------\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "    autodetect=True\n",
    ")\n",
    "\n",
    "job = client.load_table_from_dataframe(df_clean, TABLE_ID, job_config=job_config)\n",
    "job.result()  # Esperar a que termine\n",
    "\n",
    "print(\"✅ Carga completada con éxito\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
